# Glossary

**Bias**: A systematic error that leads to incorrect estimates of effect or association. Can emerge from flawed design,
 measurement, or analysis.

**Blinding**: The process of withholding knowledge of group assignment from participants, investigators, or analysts
to reduce bias. Can be single- or double-blind.

**Bootstrap**: A resampling method used to estimate statistics (like confidence intervals) from data by repeatedly
sampling with replacement. Useful for evaluating uncertainty in high-dimensional biological data.

**Case-Control Study**: An observational design comparing individuals with a specific outcome (cases) to those without
 it (controls) to identify prior exposures.

**Causal Inference**: The process of drawing conclusions about a causal relationship between variables, often through
experimental or quasi-experimental designs.

**Censoring**: A condition in survival analysis where the full event time is not observed for all subjects. Common in
clinical and longitudinal genomic studies.

**Cohort Study**: An observational design where a group is followed over time to assess how exposures affect outcomes.

**Confounding**: A hidden factor that influences both the exposure and the outcome, potentially biasing the observed
relationship and threatening causal inference.

**Construct Validity**: The extent to which a test or tool actually measures the concept it intends to measure.

**Control Group**: A group in a study that does not receive the experimental treatment and serves as a benchmark for comparison.

**Cross-Sectional Study**: A snapshot study that observes a population at a single point in time, often used to assess prevalence.

**Design**: The structured logic and architecture of how you test your ideas. Good design minimizes bias,
controls variation, and clarifies causal relationships.

**Differential Expression**: A computational method used to detect genes with statistically significant changes in
 expression between conditions (e.g., disease vs. control).

**Double-Blind**: A study design where both participants and investigators are unaware of treatment assignments to
reduce expectancy effects and bias.

**Effect Size**: A quantitative measure of the magnitude of a phenomenon. Important for interpreting the practical
 significance of results.

**Experimental**: A study design where participants or units are randomly assigned to different conditions or
 treatments to test causal effects.

**External Validity**: The degree to which study findings generalize beyond the specific sample, setting, or time in which the study was conducted.

**False Discovery Rate (FDR)**: The expected proportion of false positives among all significant results. Frequently used in genomics to adjust for multiple hypothesis testing.

**Feature Selection**: The process of selecting a subset of relevant variables (genes, proteins, etc.) for model building in high-dimensional biological data.

**Generalizability**: The extent to which results from a sample or model apply to other populations, settings, or times.

**Hidden Confounder**: An unobserved variable that correlates with both predictor and outcome, leading to biased associations. Addressed using surrogate variable analysis or latent factor models.

**Hypothesis**: A specific, testable statement about the relationship between variables. For example: “Modifier variants cluster in immune regulatory genes.”

**Internal Validity**: The degree to which observed effects can be attributed to the experimental variable rather than confounding factors or methodological artifacts.

**Knockout Model**: A genetic experimental design where a gene is intentionally disrupted or deleted to study its function or role in disease.

**Latent Variable**: A variable that is not directly observed but is inferred from other variables (e.g., unobserved batch effects in RNA-seq data).

**Matching**: A method used in observational or quasi-experimental studies to pair subjects in treatment and control groups based on shared characteristics to reduce bias.

**Meta-Analysis**: A statistical method for combining results from multiple studies to derive a more precise estimate of effect size or significance.

**Natural Experiment**: A naturally occurring situation that mimics the structure of an experiment, such as a policy change or environmental event, allowing for comparative analysis.

**Observational**: A non-interventional study design that documents relationships among variables without manipulating them. Common in epidemiology, social science, and public health.

**Overfitting**: A modeling error where a statistical model captures noise instead of the underlying signal, reducing its generalizability to new data.

**Power**: The probability that a study will detect a true effect when it exists. Depends on effect size, sample size, and significance threshold.

**Project Timeline**: A structured outline of the phases and estimated durations of the research process, including proposal, data collection, analysis, and writing.

**Proposal Abstract**: A short summary of the planned research that includes background, aims, and significance. Required in most graduate and funding proposals.

**Pseudoreplication**: The error of treating non-independent observations as independent, leading to inflated sample sizes and false positives.

**Quasi-Experimental**: A study design that compares groups with an intervention but lacks random assignment. May include pre-post comparisons or matched groups.

**Random Assignment**: A method used in experiments to allocate units to conditions purely by chance, helping to control for bias and equate groups at baseline.

**Random Forest**: A machine learning method useful for classification and regression in biological datasets, based on ensembles of decision trees.

**Random Sampling**: A method of selecting a representative subset of a population, often used to generalize findings to a broader group.

**Regression Discontinuity Design**: A quasi-experimental approach where participants are assigned to groups based on a cutoff score on a pre-intervention variable.

**Reliability**: The degree to which a measure or study produces consistent and repeatable results.

**Research Aims**: Concise statements of specific goals the project intends to achieve, often broken into Aim 1, Aim 2, etc., especially in NIH-style grants.

**Research Design**: The overall strategy for integrating different components of the study—experimental logic, sampling, analysis—to test hypotheses effectively.

**RNA-seq**: A sequencing-based method to quantify gene expression, frequently used in transcriptome analysis, disease mechanism exploration, and biomarker discovery.

**Selection Bias**: A distortion in the estimation of effect due to systematic differences in the characteristics of those selected for study groups.

**Sensitivity Analysis**: An approach to test the robustness of results to changes in model assumptions or data input. Often used to assess how parameter choices affect results.

**Single-Blind**: A design in which participants are unaware of their group assignment, but investigators are not blinded.

**Statistical Significance**: A result unlikely to have occurred by chance, according to a predefined threshold (typically p < 0.05).

**Survival Analysis**: A class of statistical methods for analyzing time-to-event data. Widely used in genomics, especially for cancer prognosis.

**Systems Biology**: An integrative field that models complex interactions between biological entities (genes, proteins, metabolites) across scales.

**Time Series Design**: A study design involving repeated observations over time—before, during, and after an intervention or event. It is often used to detect trends, interruptions, or delayed effects. Time series designs can help distinguish causal effects from background noise by analyzing change patterns across multiple time points, especially when randomization is not feasible.

**Translational Research**: Research aimed at moving discoveries from bench to bedside, such as identifying candidate biomarkers or repurposing drugs using computational approaches.

**Validation Set**: A separate subset of data used to test model generalization after training. Important in computational modeling to avoid overfitting.

**Validity**: The degree to which a method or result accurately reflects what it is intended to measure or infer.
